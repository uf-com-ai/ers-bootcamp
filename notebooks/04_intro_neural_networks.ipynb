{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d17300",
   "metadata": {},
   "source": [
    "# ![](../graphics/stat_ai_logo.png)\n",
    "***\n",
    "\n",
    "# Introduction to Artificial Neural Networks\n",
    "<img src=\"../graphics/artificial_neural_network.png\" width = \"30%\"/>\n",
    "\n",
    "### After this short module, you will be able to:\n",
    "1. Describe the biological inspiration for artificial neural networks (ANN).\n",
    "2. Explain at a basic level how the Perceptron model (one of the simplest ANNs) processes input data.\n",
    "3. Identify the important role of activation functions in ANNs.\n",
    "3. Demonstrate your understanding of the Keras API by creating a simple neural network.\n",
    "\n",
    "# ![](../graphics/get_started_icon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919db44-1f0e-4831-936e-0769ec03a103",
   "metadata": {},
   "source": [
    "# Biological Neurons\n",
    "The architecture of neural networks was inspired by the biological neuron.  In a living brain, billions of neurons are connected together, forming a dense and complex network. \n",
    "\n",
    "Biological neurons...\n",
    "* use electrical and chemical signals to pass information between different regions of the brain\n",
    "* receive information through their dendrites and cell body\n",
    "* transmit eletrical signals down their axons which triggers the release of neurotransmitters through their axon terminals.\n",
    "\n",
    "The electrical signals sent down the axon **do not** vary in magnitude.\n",
    "\n",
    "<center><img src=\"../graphics/biological_neuron.png\" width=\"70%\" alt='biological neuron'/></center>\n",
    "<center><a href=\"https://www.upgrad.com/blog/biological-neural-network/\">Source</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5407a734-876c-4b4c-afed-bcc24796bbdc",
   "metadata": {},
   "source": [
    "# Perceptron / Artificial Neuron\n",
    "\n",
    "A perceptron is a type of artificial neuron or computational unit used in artificial neural networks. A perceptron - also known as a *node* in deep learning - contains two functions, a net input function that sums up incoming inputs and an **activation** function. \n",
    "\n",
    "Perceptrons...\n",
    "* are one of the simplest ANNs.\n",
    "* were inspired by biological neurons.\n",
    "* receive information through the lines (weights) displayed to the left.\n",
    "* apply an activation function to their received signal and output the result.\n",
    "\n",
    "The numerical signals that are output **do** vary in magnitude!\n",
    "\n",
    "<center><img  src=\"../graphics/perceptron.png\" width=\"500\" height=\"500\" alt='biological neuron'/></center>\n",
    "\n",
    "<center><a href=\"https://wiki.pathmind.com/neural-network\">Source</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d0db19",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "The choice of activation function is important as it can have a major impact on how well your model trains. Also, the non-linear transformation applied by activation functions allows networks to model a wide variety of non-linear problems.\n",
    "\n",
    "Activation functions..\n",
    "* are simple functions that transforms input values.\n",
    "* are used in ANNs to map neuron inputs to neuron outputs (also called neuron *activations*).\n",
    "\n",
    "<center><img width=\"500\" src=\"../graphics/activation_functions.webp\"/></center>\n",
    "<center><a href=\"https://machine-learning.paperspace.com/wiki/activation-function\">Source</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38016ce3",
   "metadata": {},
   "source": [
    "#### ![](../graphics/exercise_icon.png)\n",
    "Given a single neuron with input **x**, write a simple **linear activation** function using two lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf3f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e19cd29",
   "metadata": {},
   "source": [
    "#### ![](../graphics/exercise_icon.png)\n",
    "Given a single neuron with input **x**, write a simple **ReLU activation** function using two lines of code.\n",
    "\n",
    "Hint: we can use the built-in Python function max(). When you pass two numbers a and b into the max function as max(a,b), the function will return the larger value. *Examples:* <code>max(-2, 3) = 3</code> or <code>max(1.5, 1.1) = 1.5</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d33784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f01e3b9",
   "metadata": {},
   "source": [
    "Is the ReLU a **linear** or **non-linear** activation function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000d42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b20657-0b9e-4ff2-89c7-91bc26342d78",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANNs)\n",
    "An artificial neural network (ANN) is comprised of artificial neurons connected together in layers. \n",
    "\n",
    "ANNs are...\n",
    "* a subset of machine learning that can be used to model a wide range of problems.\n",
    "* composed of a collection of interconnected connected nodes that accept an input and produce an output.\n",
    "* a fundamental component of deep learning.\n",
    "\n",
    "<center><img src=\"../graphics/neural_network_im.jpg\" width=\"55%\" alt=\"Neural Network\" style=\"background:white;\"/></center>\n",
    "\n",
    "<center><a href=\"https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks\">Source</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f66ab33-169f-46c3-b901-d296cdf39b97",
   "metadata": {},
   "source": [
    "# Training a Neural Network\n",
    "\n",
    "Training a neural network is the process of teaching the model to perform a specific task or learn a particular pattern from a given set of training data. It involves adjusting the model's internal parameters or weights to minimize the difference between its predictions and the desired outputs.\n",
    "\n",
    "Assuming that training data has been prepared, the training process includes the following steps:\n",
    "\n",
    "1. **Forward Propagation:** The training data is fed into the model, and its input features are multiplied by the weights and passed through activation functions in each layer. \n",
    "2. **Loss (Error) Calculation:** The loss function calculates the network's total error, the difference between the model's predictions and the actual target values.\n",
    "3. **Backpropagation:** The gradients of the loss function with respect to the model's parameters are computed using the chain rule of calculus. \n",
    "4. **Parameter / Weight Update:**  The model's parameters are updated using optimization algorithms like stochastic gradient descent (SGD) or its variants.\n",
    "5. **Iterate:** Repeat the first four steps for each training run or epoch.  Each time we run through the entire training dataset, we say that we trained for one *epoch*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a10fc-ee69-43df-a66b-c5aaf71a82ed",
   "metadata": {},
   "source": [
    "## Creating an Artificial Neural Network (Multi-Layer Perceptron)\n",
    "* An important part of an AI research project is evaluating different models to identify which works best for a given dataset and task.\n",
    "* Let's train a deep learning model on the dataset from our machine learning notebook.\n",
    "* This time, we'll be using a deep learning (DL) model that uses an artificial neural network called the multi-layer perceptron (MLP).\n",
    "* We will be using the user-friendly Python library `Keras` to build our MLP model.\n",
    "\n",
    "#### ![](../graphics/note_icon.png)\n",
    ">A full description of DL or the MLP is beyond the scope of this lesson. We refer interested students to [Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press](https://www.deeplearningbook.org).\n",
    "\n",
    "Important bullet points for this module:\n",
    "* Deep learning was biologically inspired.\n",
    "* A DL model like the MLP is often defined in terms of **layers** (one *input layer*, one or more *hidden layers*, and one *output layer*). The more hidden layers that are added to a model, the \"deeper\" it gets.\n",
    "* Each hidden layer is designed to transform the data from the layer before.\n",
    "* The final layer learns to predict an outcome based on all of these transformations.\n",
    "\n",
    "<center><img src=\"../graphics/ann.jpg\" alt=\"ANN\" width=\"400\" height=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d578aa",
   "metadata": {},
   "source": [
    "* First, let's load our dataset and split it into training and testing sets. (We saw this in the previous notebook, so we'll skip the explanation).\n",
    "* Just to change things up a bit, let's use **70% of the dataset to train** our model, and **30% of the dataset to test** our model. (Last time we used 80% for training and 20% for testing.)\n",
    "    * We will accomplish this in the `train_test_split` function by changing the `test_size` parameter to be equal to `0.3`, representing 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee8110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('https://www.dropbox.com/s/smxqzfipqc7bznf/data_small_processed.csv?dl=1')\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09495a56-0d85-4e70-b3e0-84e7584baae0",
   "metadata": {},
   "source": [
    "Let's create our first MLP model with the following specification:\n",
    "* One input layer with 47 units (one per input variable)\n",
    "* One hidden layer with 128 units\n",
    "* Another hidden layer with 64 units\n",
    "* An output layer with 1 unit (for predicting 0 or 1 corresponding to our AKI outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4d3e1a-68fe-4c37-9a6e-addcc00ed4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 19:31:45.660406: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 19:31:47.416265: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "# Add the first hidden layer with 128 neurons (and implicitly create the input layer by specifying the \"input_dim\")\n",
    "model.add(layers.Dense(units=128, input_dim=47, activation='relu'))\n",
    "\n",
    "# Add the second hidden layer with 64 neurons\n",
    "model.add(layers.Dense(units=64, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model for a classification problem such as ours.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2054713-5ac3-4685-b617-a521ed0f02b7",
   "metadata": {},
   "source": [
    "### ðŸ’¬ (Optional) Code Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4827ad1-15b0-47f3-bb92-1eb20471d1c5",
   "metadata": {},
   "source": [
    "Each hidden layer of our neural network will be created using the **Dense** class from Keras. For each layer, we must define the number of hidden units (also known as neurons). There are several optional arguments we may also pass, which can be viewed in the [Keras documentation page](https://keras.io/api/layers/core_layers/dense/). We can add many layers to our deep learning model using the .add() function of the Sequential class. You can think of a Sequential container as a list of hidden layers.\n",
    "\n",
    "For the first layer of our neural network, we must tell Keras how many variables to expect in each input vector. From our previous data exploration, we know that each patient is defined by 47 different variables, so the input dimension to our network is 47.\n",
    "\n",
    "One reason why deep learning models are so powerful is their ability to model complex variable interactions through nonlinear activation functions. We have several choices for activation function. In our example, we will use the commonly chosen Rectified Linear Unit activation (ReLU).\n",
    "\n",
    "Once we are satisfied with the hidden layers of our model, we need to add an output layer for generating class predictions. Our output layer will also be a Dense layer, but it will only have a single (1) unit. Instead of ReLU, we will use a sigmoid activation function, which is typically chosen for binary classification problems such as ours. Using a sigmoid activation on our output layer allows us to interpret the output as a prediction probability. In other words, the probability that a given input vector belongs to class 1.\n",
    "\n",
    "Now that we have defined the architecture of our neural network, we will use the .compile() function to build it. In our example we are defining a few arguments that are associated with the training of our model:\n",
    "* We are using a binary cross-entropy loss. This is an appropriate choise for binary classification.\n",
    "* We will be using the Adam optimizer, which is a popular version of stochastic gradient descent (SGD).\n",
    "* For this example, we are interested in our model's prediction accuracy, so we'll tell Keras to use the \"accuracy\" metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74d1b6-52f0-4c1a-9227-496b5bd665f9",
   "metadata": {},
   "source": [
    "### Training our MLP model\n",
    "\n",
    "* Now it's time to train our prediction model!\n",
    "* We will train (also called \"fit\") the model using our training dataset we already created.\n",
    "* We will use the one-line function `.fit()` to train our entire deep learning model.\n",
    "* We will specify some additional parameters to be used during the training process:\n",
    "    * We will tell Keras to train the model for 10 epochs.\n",
    "    * We will use a batch size of 64 samples. During each epoch, the model will pass in 64 samples at a time.\n",
    "    * We will use a random 30% of the training dataset as our **validation set** (different from the test set) for computing metrics while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768842f8-6a8d-42ae-8971-48495ca7e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5168 - accuracy: 0.7776 - val_loss: 0.4666 - val_accuracy: 0.8357\n",
      "Epoch 2/10\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4423 - accuracy: 0.8357 - val_loss: 0.4667 - val_accuracy: 0.8357\n",
      "Epoch 3/10\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4295 - accuracy: 0.8357 - val_loss: 0.4677 - val_accuracy: 0.8357\n",
      "Epoch 4/10\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4187 - accuracy: 0.8357 - val_loss: 0.4700 - val_accuracy: 0.8357\n",
      "Epoch 5/10\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4070 - accuracy: 0.8357 - val_loss: 0.4717 - val_accuracy: 0.8357\n",
      "Epoch 6/10\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.3939 - accuracy: 0.8359 - val_loss: 0.4767 - val_accuracy: 0.8357\n",
      "Epoch 7/10\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.3776 - accuracy: 0.8367 - val_loss: 0.4821 - val_accuracy: 0.8352\n",
      "Epoch 8/10\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.3607 - accuracy: 0.8410 - val_loss: 0.4902 - val_accuracy: 0.8314\n",
      "Epoch 9/10\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.3387 - accuracy: 0.8473 - val_loss: 0.5087 - val_accuracy: 0.8319\n",
      "Epoch 10/10\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.3186 - accuracy: 0.8584 - val_loss: 0.5222 - val_accuracy: 0.8190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff4ac55b2b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e764a18-057e-4b51-b133-0b510f175e9e",
   "metadata": {},
   "source": [
    "**Done!**\n",
    "\n",
    "* Let's check the performance of our trained model on the test set we already created.\n",
    "* The model has never seen this particular data, so it can provide an idea of how well the model might perform in the future (***generalizability to unseen data***).\n",
    "* We will use the Keras function `.evaluate()`, which will compute the loss, as well as any metrics we defined when compiling our model.\n",
    "    * Since we told Keras to use \"accuracy\" when we compiled the model, we will see the model's accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae39541-c61c-4d6c-9eb7-ecc49487da2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 555us/step - loss: 0.5175 - accuracy: 0.8150\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e396c-5045-43c5-87e8-b269e597eaf9",
   "metadata": {},
   "source": [
    "#### ![](../graphics/exercise_icon.png)\n",
    "Create an MLP with **3** hidden layers (instead of 2), with the following number of neurons in each:\n",
    "* Hidden layer 1: 512 neurons\n",
    "* Hidden layer 2: 256 neurons\n",
    "* Hidden layer 3: 128 neurons\n",
    "\n",
    "Create, compile, train, and evaluate this new MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc54a17e-0cb9-4721-91e2-87ee993fc26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it! (feel free to re-use most of the code from before!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ef5be7-e23a-4ead-ab76-38b4e14aa083",
   "metadata": {},
   "source": [
    "#### ![](../graphics/exercise_icon.png) \n",
    "Create an MLP with **8** hidden layers (instead of 2), with **any number** of neurons in each hidden layer. Create, compile, train, and evaluate this new MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5489bbbe-7e1d-48f3-8371-84f0ad2fbade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e299b1",
   "metadata": {},
   "source": [
    "***\n",
    "## Bonus Exercise (Featured in Presentation)\n",
    "\n",
    "#### ![](../graphics/tensorflow_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5513c56f",
   "metadata": {},
   "source": [
    "### Interactive Demo: TensorFlow <a href='https://playground.tensorflow.org/#activation=linear&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=1&seed=3.17465&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false'>Neural Network Playground</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd6d20-a1fb-4413-9b82-27481f3f7ee1",
   "metadata": {},
   "source": [
    "***\n",
    "#### Attribution\n",
    "Some content in this learning experience was adapted from the University of Florida [Practicum AI](https://practicumai.org) \"Fundamentals of Deep Learning\" course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
